


老实说，职业生涯中，专门搞OS搞内核的上班工作时间不多，相关的虚拟化更少。今晚抱佛脚研究了一下vmware的实现，纯理论，赶紧写，不然忘了。

最初的目标，让guest OS及其上面的进程的指令直接执行。但是要先为其建立隔离的环境。以前拿vmware调试过linux内核，貌似在gdb下虚拟机内核呈现为每CPU的进程(还是线程?)。因此虚拟机在VMM和host OS看来就是进程了。进程的页表呢，guest OS上进程间切换时

要切换页表，而guest OS内核页表不会变。因此guest OS对应的host进程的页表(除掉内核页表)即是guest OS中的当前进程的页表，但是其中要分出guest OS内核页表了，并且权限要设置成内核权限(同host OS内核页表)。当guestOS上的进程调用syscall时，指令被VMM

截获(设置断点，类似GDB)，VMM将gusetOs内核页表权限设置成进程权限(同host OS进程页表)，同时要建立系统调用环境，即guestOS进程和内核使用同一个栈，将断点指令改写成直接call到guest OS的syscall函数的指令，参数传递依旧(不同于物理机环境的是，内核栈

与用户栈同一个，内核内存和进程内存访问权限都是用户态，靠VMM保护!), 执行完syscall后还原(内存权限，断点指令)，中间可能要调度和切换（参见调度）

中断和IO,以桥接网卡和协议栈为例
host网卡收到包后(此时显然guest OS被VMM/HOST抢占而中断而挂起)，包进入内核网桥(或者ovs,但是不知道OVS有没有改内核，还是直接增加一个内核模块即OK,反正这里内核网桥要做一点改动)
  1  网桥根据虚拟机注册的虚拟网卡mac转发到虚拟网卡，实际上是通过写虚拟网卡的DMA buffer以及其相应的寄存器标志等控制值实现的，当然DMA buffer是guset OS的内核地址
  2  向guestOS注入中断，其实现是改变guest OS的堆栈，象linux的signal处理(handle_signal函数)一样在guest OS当前运行的栈中插入guest OS的对应网卡中断处理函数:
           当guest OS被VMM中断后恢复运行时就先执行中断函数，该函数会调度软中断，然后执行软中断，然后可能要调度和切换（参见调度）,目前暂时认为他们对VMM/HOST透明,即直接一条条指令执行(按照guest内核页表和guest OS指令)

信号处理
 对VMM/host透明

进程调度和切换
    进程调度对VMM/host透明
    切换时，假设guestOS是linux，则需要切换页表和堆栈。
           1 切换页表时又会陷入VMM, VMM将guesOS对应的（在host中的）进程页表中的guest OS的当前进程的页表映射关系修改为待切换到的进程的页表，
           2 guest OS内核堆栈指针的切换则对VMM透明
           实际上整个guest系统(OS和进程)使用的内存就是host中的对应的进程(guest系统)，页表也是这样的对应关系

    同物理机一样，随后一路返回到guest OS进程，
         只是对于系统调用返回到进程前，需要VMM修改guest OS的内核页表权限为内核权限,断点指令也需恢复为系统调用

     
   

问题


1 内核网桥(或者ovs,但是不知道OVS有没有改内核，还是直接增加一个模块即OK,反正这里内核网桥要做一点改动，或者是使用tap接口？)
2 VMM和host OS关系？这里假设VMM运行在host内核中(或者直接改host linux内核)，对guest OS有绝对控制权,可以随时中断之
3 只是对于系统调用返回到进程前，需要VMM修改guest OS的内核页表权限为内核权限
        如何陷入VMM?
